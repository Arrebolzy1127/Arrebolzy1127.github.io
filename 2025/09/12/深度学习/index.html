<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Hexo | Hexo</title><meta name="author" content="zy"><meta name="copyright" content="zy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习1.深度学习概括1.1深度学习与机器学习的区别​	特征处理：机器学习的特征工程是靠手动完成的，深度学习通过多个层组成，他们通常将更简单的模型组合在一起，通过将数据从一层传递到另一层来构建更复杂的模型。通过大量数据的训练自动得到模型，而不要人工设计特征提取环节。（适合用与难提取特征的图像、语音、自然语言领域） 1.2深度学习代表算法——神经网络1.2.1神经网络​	人工神经网络简称ANN，也">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="深度学习1.深度学习概括1.1深度学习与机器学习的区别​	特征处理：机器学习的特征工程是靠手动完成的，深度学习通过多个层组成，他们通常将更简单的模型组合在一起，通过将数据从一层传递到另一层来构建更复杂的模型。通过大量数据的训练自动得到模型，而不要人工设计特征提取环节。（适合用与难提取特征的图像、语音、自然语言领域） 1.2深度学习代表算法——神经网络1.2.1神经网络​	人工神经网络简称ANN，也">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-12T02:43:38.958Z">
<meta property="article:modified_time" content="2025-09-04T09:58:26.803Z">
<meta property="article:author" content="zy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "url": "http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2025-09-12T02:43:38.958Z",
  "dateModified": "2025-09-04T09:58:26.803Z",
  "author": [
    {
      "@type": "Person",
      "name": "zy",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hexo',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hexo</span></a><a class="nav-page-title" href="/"><span class="site-name">Hexo</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">无标题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-09-12T02:43:38.958Z" title="Created 2025-09-12 10:43:38">2025-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-09-04T09:58:26.803Z" title="Updated 2025-09-04 17:58:26">2025-09-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="1-深度学习概括"><a href="#1-深度学习概括" class="headerlink" title="1.深度学习概括"></a>1.深度学习概括</h2><h3 id="1-1深度学习与机器学习的区别"><a href="#1-1深度学习与机器学习的区别" class="headerlink" title="1.1深度学习与机器学习的区别"></a>1.1深度学习与机器学习的区别</h3><p>​	<strong>特征处理：</strong>机器学习的特征工程是靠手动完成的，深度学习通过多个层组成，他们通常将更简单的模型组合在一起，通过将数据从一层传递到另一层来构建更复杂的模型。通过大量数据的训练自动得到模型，而不要人工设计特征提取环节。（适合用与难提取特征的图像、语音、自然语言领域）</p>
<h3 id="1-2深度学习代表算法——神经网络"><a href="#1-2深度学习代表算法——神经网络" class="headerlink" title="1.2深度学习代表算法——神经网络"></a>1.2深度学习代表算法——神经网络</h3><h4 id="1-2-1神经网络"><a href="#1-2-1神经网络" class="headerlink" title="1.2.1神经网络"></a>1.2.1神经网络</h4><p>​	人工神经网络简称ANN，也简称为NN，是一种模仿生物神经网络（动物中枢神经系统,大脑）结构和功能的<strong>计算模型</strong>。经典的神经网络结构包含三个层次的神经网络。分别是<strong>输入层，输出层以及隐藏层</strong>。</p>
<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250831154045603.png" alt="image-20250831154045603" style="zoom: 50%;" />

<h4 id="其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出-逻辑回归），输入层的神经元只是输入。"><a href="#其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出-逻辑回归），输入层的神经元只是输入。" class="headerlink" title="其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出(逻辑回归），输入层的神经元只是输入。"></a>其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出(逻辑回归），输入层的神经元只是输入。</h4><p>​	<strong>神经网络的特点：</strong></p>
<ul>
<li>每个连接都有个权重，同一层神经元之间没有连接</li>
<li>神经元当中会含有激活函数</li>
<li>最后的输出结果对应的层也称之为全连接层</li>
</ul>
<h3 id="1-3神经网络基础"><a href="#1-3神经网络基础" class="headerlink" title="1.3神经网络基础"></a>1.3神经网络基础</h3><h4 id="1-3-1Logistic回归"><a href="#1-3-1Logistic回归" class="headerlink" title="1.3.1Logistic回归"></a>1.3.1Logistic回归</h4><p>​	逻辑回归是一个主要用于二分类分类的算法，给定一个x输出一个该样本属于1对应类别的预测概率</p>
<h4 id="1-3-2损失函数"><a href="#1-3-2损失函数" class="headerlink" title="1.3.2损失函数"></a>1.3.2损失函数</h4><p>​	<strong>损失函数（loss fuction）</strong>用于衡量预测结果与真实值之间的误差，最简单的损失函数定义方式为平方根损失，根据损失函数来优化参数w和b。</p>
<p>​	损失函数是在<strong>单个</strong>训练样本中定义的，它衡量了在单个训练样本上的表现。<strong>代价函数</strong>（cost fuction）是在<strong>全体</strong>训练样本上的表现，衡量参数w和b的效果，所有训练样本的损失平均值。</p>
<h4 id="1-3-3梯度下降算法"><a href="#1-3-3梯度下降算法" class="headerlink" title="1.3.3梯度下降算法"></a>1.3.3梯度下降算法</h4><p>​	<strong>目标：</strong>使损失函数的值找到最小值</p>
<p>​	<strong>方式：</strong>梯度下降</p>
<p>​	函数的梯度（gradient）指出了函数的最陡增长方向。<strong>梯度的方向走，函数增长的越快，按照梯度的负方向走，函数值降低的最快。</strong>模型的训练目标是寻找合适的w和b以最小化代价函数值。假设w与b都是一维实数，那么可以得到如下的J关于w与b的图：</p>
<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250831165921477.png" alt="image-20250831165921477" style="zoom:33%;" />

<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250831211103916.png" alt="image-20250831211103916" style="zoom:67%;" />

<h4 id="1-3-4向量化编程"><a href="#1-3-4向量化编程" class="headerlink" title="1.3.4向量化编程"></a>1.3.4向量化编程</h4><p>​	<strong>梯度下降m个样本的梯度计算：</strong>每更新一次梯度时候，在训练期间我们会拥有m个样本，那么每个样本提供进去都可以做一个梯度下降计算。所以我们要去做在所有样本上的计算结果、梯度等操作。</p>
<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250902134117410.png" alt="image-20250902134117410" style="zoom:80%;" />

<p>​	<strong>向量化优势：</strong>进行计算的时候，不要使用for循环进行计算，有Numpy可以进行更加快速的向量化计算。</p>
<p>​	避免使用for进行向量的计算，使用np.dot等numpy工具进行操作</p>
<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250903104718760.png" alt="image-20250903104718760" style="zoom: 50%;" />

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">random与numpy.random的区别</span><br><span class="line">🐍 1. random （标准库）</span><br><span class="line">属于 Python 内置标准库，不依赖第三方库。</span><br><span class="line">主要用于 简单的随机数生成，比如抽样、打乱、生成随机整数/浮点数。</span><br><span class="line">速度相对慢，不适合生成大规模随机数组。</span><br><span class="line">随机数是 逐个生成 的，不能直接处理向量。</span><br><span class="line">📌 示例：</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">print(random.randint(1, 10))     # 1~10 的随机整数</span><br><span class="line">print(random.random())           # 0~1 的随机浮点数</span><br><span class="line">print(random.choice([&#x27;A&#x27;, &#x27;B&#x27;])) # 随机选择</span><br><span class="line">🔢 2. numpy.random （NumPy 库）</span><br><span class="line">属于 NumPy 库，专为 科学计算和向量化 设计。</span><br><span class="line">能够一次性生成 大规模随机数组，非常高效，速度远快于 random。</span><br><span class="line">提供了 更多分布函数：均匀分布、正态分布、二项分布、泊松分布等。</span><br><span class="line">结果是 NumPy 数组，可以直接参与矩阵运算，非常适合深度学习。</span><br><span class="line">📌 示例：</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">print(np.random.randint(1, 10, size=5))   # 生成5个1~9的随机整数</span><br><span class="line">print(np.random.rand(3, 2))               # 3x2 的 [0,1) 随机浮点数</span><br><span class="line">print(np.random.rand(12))                 # 1×12的 [0,1) 随机浮点数(均匀分布)</span><br><span class="line">print(np.random.randn(3, 2))              # 3x2 的正态分布随机数</span><br><span class="line">=======================================================================</span><br><span class="line">⚡ 性能对比</span><br><span class="line">import random</span><br><span class="line">import numpy as np</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># random</span><br><span class="line">t1 = time.time()</span><br><span class="line">lst = [random.random() for _ in range(10**6)]</span><br><span class="line">t2 = time.time()</span><br><span class="line"></span><br><span class="line"># numpy.random</span><br><span class="line">arr = np.random.rand(10**6)</span><br><span class="line">t3 = time.time()</span><br><span class="line"></span><br><span class="line">print(&quot;random 用时：&quot;, t2 - t1)</span><br><span class="line">print(&quot;numpy.random 用时：&quot;, t3 - t2)</span><br><span class="line">👉 结果一般是 numpy.random 快 几十倍，因为它底层用的是 C 实现 + 向量化。</span><br><span class="line">=======================================================================补充，rand和randn区别也就是均匀分布和正态分布区别</span><br><span class="line"> np.random.rand(10)</span><br><span class="line">分布类型：均匀分布 U(0,1)</span><br><span class="line">区间范围：[0, 1)</span><br><span class="line">含义：每个数落在 [0,1) 内的概率相同</span><br><span class="line">常用于：初始化概率值、随机采样</span><br><span class="line">📌 示例：</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">print(np.random.rand(10))</span><br><span class="line">可能输出：[0.12 0.87 0.45 0.23 0.91 0.34 0.65 0.07 0.56 0.78]</span><br><span class="line">2️⃣ np.random.randn(10)</span><br><span class="line">分布类型：标准正态分布 N(0,1)</span><br><span class="line">区间范围：理论上 (-∞, +∞)，但大多数值落在 [-3,3]</span><br><span class="line">均值：0</span><br><span class="line">标准差：1</span><br><span class="line">常用于：深度学习权重初始化、噪声生成</span><br><span class="line">📌 示例：</span><br><span class="line"></span><br><span class="line">print(np.random.randn(10))</span><br><span class="line">可能输出：</span><br><span class="line"></span><br><span class="line">[ 0.14 -1.03  0.56  2.17 -0.45  0.09 -0.72 -0.11  1.28 -0.33]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import scipy.stats as st</span><br><span class="line"></span><br><span class="line">p1 = st.norm.cdf(1) - st.norm.cdf(-1)  # 在 [-1,1] 的概率</span><br><span class="line">p2 = st.norm.cdf(2) - st.norm.cdf(-2)  # 在 [-2,2] 的概率</span><br><span class="line">p3 = st.norm.cdf(3) - st.norm.cdf(-3)  # 在 [-3,3] 的概率</span><br><span class="line"></span><br><span class="line">print(p1, p2, p3)</span><br><span class="line">输出：</span><br><span class="line">0.6827   # 68.27% 在 [-1,1]</span><br><span class="line">0.9545   # 95.45% 在 [-2,2]</span><br><span class="line">0.9973   # 99.73% 在 [-3,3]</span><br><span class="line">====================================================================</span><br><span class="line">cdf是累计分布函数（随机变量X小于等于某个值的概率</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">补充scipy.stats.norm常用用法(正态分布的常见操作)</span><br><span class="line">1.PDF（概率密度函数）</span><br><span class="line">from scipy import stats as st</span><br><span class="line">print(st.norm.pdf(0))</span><br><span class="line">给出该点的概率密度值（不是概率，而是曲线高度）</span><br><span class="line">2.cdf(累积分布函数)</span><br><span class="line">print(st.norm.cdf(1))   # P(X ≤ 1) = 0.8413</span><br><span class="line">给出随机变量 ≤ x 的累计概率。</span><br><span class="line">3.ppf（百分位点数/逆CDF）</span><br><span class="line">print(st.norm.ppf(0.975))   # 1.95996...</span><br><span class="line">反函数：已知概率，求对应的分位点。</span><br><span class="line">比如 0.975 对应的分位点是 1.96（常用于置信区间）。</span><br><span class="line">4.sf（生存函数）</span><br><span class="line">print(st.norm.sf(1))   # P(X &gt; 1) = 0.1587</span><br><span class="line">sf(x) = 1 - cdf(x)，表示大于某个值的概率。</span><br><span class="line">5. ISF（逆生存函数）</span><br><span class="line">print(st.norm.isf(0.025))   # 1.95996...</span><br><span class="line">isf(q) = ppf(1-q)，常和 sf 搭配使用。</span><br><span class="line">6. RVS（随机采样）</span><br><span class="line">print(st.norm.rvs(size=5))   # 生成5个 N(0,1) 随机数</span><br><span class="line">用于模拟数据或采样。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>方法</th>
<th>含义</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td><code>pdf(x)</code></td>
<td>概率密度函数 f(x)</td>
<td>看曲线高度</td>
</tr>
<tr>
<td><code>cdf(x)</code></td>
<td>累积分布函数 P(X ≤ x)</td>
<td>算区间概率</td>
</tr>
<tr>
<td><code>ppf(q)</code></td>
<td>百分位点函数（逆cdf）</td>
<td>由概率求分位点</td>
</tr>
<tr>
<td><code>sf(x)</code></td>
<td>生存函数 P(X &gt; x)</td>
<td>右尾概率</td>
</tr>
<tr>
<td><code>isf(q)</code></td>
<td>逆生存函数</td>
<td>由右尾概率求分位点</td>
</tr>
<tr>
<td><code>rvs(size)</code></td>
<td>采样</td>
<td>生成随机数</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.ones or np.zeros#全为1或者0的矩阵</span><br><span class="line">np.exp#指数运算</span><br><span class="line">np.log#对数运算</span><br><span class="line">np.abs#绝对值运算</span><br></pre></td></tr></table></figure>

<p><strong>多个样本向量化运算</strong></p>
<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250903151446901.png" alt="image-20250903151446901" style="zoom: 50%;" />

<img src="C:\Users\99611\AppData\Roaming\Typora\typora-user-images\image-20250903153109840.png" alt="image-20250903153109840" style="zoom:67%;" />

<h4 id="1-3-5正向传播与反向传播"><a href="#1-3-5正向传播与反向传播" class="headerlink" title="1.3.5正向传播与反向传播"></a>1.3.5正向传播与反向传播</h4><p>​	一个是从前往后的计算出梯度与损失，另外一部分是从后往前计算参数的更新梯度值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A @ B   # 矩阵乘法</span><br><span class="line">A * B   # 按元素相乘</span><br><span class="line">np.dot也是矩阵相乘</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">x.shape输出为（209，64，32，3）</span><br><span class="line">如果采用 NHWC 格式（常见于 TensorFlow/Keras）：</span><br><span class="line">209 → 批量大小（batch size），也就是数据集中有 209 张图片</span><br><span class="line">64 → 高度（height），每张图片有 64 个像素点行</span><br><span class="line">32 → 宽度（width），每张图片有 32 个像素点列</span><br><span class="line">3 → 通道数（channels），这里是 RGB 三通道彩色图，1的话就是黑白图</span><br><span class="line"></span><br><span class="line">train_x = train_x.reshape(train_x.shape[0], -1).T</span><br><span class="line">train_x.shape[0] = 209 → 表示有 209 张图片</span><br><span class="line">-1 → 让 NumPy 自动计算剩余维度展平</span><br><span class="line">这里每张图片是 64*32*3 = 6144 个像素</span><br><span class="line">所以 reshape 后变成 (209, 6144)</span><br><span class="line">.T → 转置，得到 (6144, 209)</span><br><span class="line">这种格式常用于神经网络输入，每一列是一张展平后的图片</span><br><span class="line"></span><br><span class="line">=======================================================================🔹 train_x</span><br><span class="line">训练特征数据 (features)</span><br><span class="line">存放的是输入样本，比如图像、文本、语音等。</span><br><span class="line">举例：</span><br><span class="line">对于图像分类：</span><br><span class="line">train_x.shape = (209, 64, 32, 3) → 表示有 209 张图片，每张是 64×32 像素，RGB 三通道</span><br><span class="line">如果展平后：</span><br><span class="line">train_x.shape = (6144, 209) → 每一列是一张图片（6144 = 64×32×3）</span><br><span class="line">🔹 train_y</span><br><span class="line">训练标签 (labels)</span><br><span class="line">存放的是每个样本对应的正确答案（监督学习里的目标）。</span><br><span class="line">举例：</span><br><span class="line">如果是二分类任务（比如“猫 vs 非猫”）：</span><br><span class="line">train_y.shape = (1, 209)</span><br><span class="line">值可能是 0 或 1</span><br><span class="line">如果是多分类任务（比如 10 个类别的数字识别）：</span><br><span class="line">train_y.shape = (209,) 或 One-hot 形式 (209, 10)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def propagate(w,b,X,Y)</span><br><span class="line">	m = X.shape[1]</span><br><span class="line">	#前向传播w(n,1),x(n,m)</span><br><span class="line">	A = basic_sigmod(np.dot(w.T,X)+b)</span><br><span class="line">	#计算损失</span><br><span class="line">	cost = -1/m*np.sum(Y*np.log(A)+(1-Y)*log(1-A))</span><br><span class="line">	#反向传播</span><br><span class="line">	dz = A-Y</span><br><span class="line">	dw = 1/m*np.dot(X,dz.T)</span><br><span class="line">	db = 1/m*np.sum(dz)</span><br><span class="line">#优化</span><br><span class="line">def optimize(w,b,X,Y,num_iterations,learning_rate)</span><br><span class="line">	#num_iterations:迭代次数</span><br><span class="line">	#更新后</span><br><span class="line">	#w = w - 学习率*dw</span><br><span class="line">	#b = b - 学习率*db</span><br><span class="line">	costs - []</span><br><span class="line">	for i in range(num_iterations):</span><br><span class="line">		grades,cost = propagate(w,b,X,Y)</span><br><span class="line">		dw = grades[&quot;dw&quot;]</span><br><span class="line">		db = grades[&quot;db&quot;]</span><br><span class="line">		 </span><br><span class="line">		w = w-learning_rate*dw</span><br><span class="line">		b = b-learning_rate*db</span><br><span class="line">		#结束</span><br><span class="line">		if i % 100 == 0:</span><br><span class="line">			costs.append(cost)</span><br><span class="line">		if i % 100 == 0:</span><br><span class="line">			print(&quot;损失结果 %i:%f&quot;%（i，cost))#%i占位符，表示整数(和%d等价)</span><br></pre></td></tr></table></figure>

<h3 id="1-4浅层神经网络"><a href="#1-4浅层神经网络" class="headerlink" title="1.4浅层神经网络"></a>1.4浅层神经网络</h3><h4 id="1-4-1浅层神经网络表示"><a href="#1-4-1浅层神经网络表示" class="headerlink" title="1.4.1浅层神经网络表示"></a>1.4.1浅层神经网络表示</h4></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">zy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/09/12/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Hello World</div></div><div class="info-2"><div class="info-item-1">Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot;  More info: Writing Run server1$ hexo server  More info: Server Generate static files1$ hexo generate  More info: Generating Deploy to remote sites1$ hexo deploy  More info: Deployment </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zy</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E6%8B%AC"><span class="toc-number">1.1.</span> <span class="toc-text">1.深度学习概括</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1深度学习与机器学习的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E8%A1%A8%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2深度学习代表算法——神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">1.2.1神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%B8%AD%E6%AF%8F%E5%B1%82%E7%9A%84%E5%9C%86%E5%9C%88%E4%BB%A3%E8%A1%A8%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%8C%E9%9A%90%E8%97%8F%E5%B1%82%E5%92%8C%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E6%9C%89%E8%BE%93%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97%E5%90%8E%E8%BE%93%E5%87%BA-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%89%EF%BC%8C%E8%BE%93%E5%85%A5%E5%B1%82%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E5%8F%AA%E6%98%AF%E8%BE%93%E5%85%A5%E3%80%82"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出(逻辑回归），输入层的神经元只是输入。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1Logistic%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">1.3.1Logistic回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">1.3.2损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">1.3.3梯度下降算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-4%E5%90%91%E9%87%8F%E5%8C%96%E7%BC%96%E7%A8%8B"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">1.3.4向量化编程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-5%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.1.3.5.</span> <span class="toc-text">1.3.5正向传播与反向传播</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4浅层神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">1.4.1浅层神经网络表示</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="无标题">无标题</a><time datetime="2025-09-12T02:43:38.958Z" title="Created 2025-09-12 10:43:38">2025-09-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/12/hello-world/" title="Hello World">Hello World</a><time datetime="2025-09-12T01:55:20.445Z" title="Created 2025-09-12 09:55:20">2025-09-12</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By zy</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>