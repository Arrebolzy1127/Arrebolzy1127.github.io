<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习第二节（神经网络与模型训练过程） | zy de 小破站</title><meta name="author" content="zy"><meta name="copyright" content="zy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习第二节（神经网络与模型训练过程）1.人工神经网络基础​	ANN是张量（权重，weights）和数学运算的集合，将一个或者多个张量作为输入并预测相应输出。将输入连接到输出的操作方式称为神经网络的架构，我们可以根据不同的任务构建不同架构，即基于问题是包含结构化数据还是非结构化数据（图像、文本、语言）数据，这些数据就是输入和输出张量的列表。ANN由以下部分组成： ​	输入层：将自变量作为输入 ​">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习第二节（神经网络与模型训练过程）">
<meta property="og:url" content="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/index.html">
<meta property="og:site_name" content="zy de 小破站">
<meta property="og:description" content="深度学习第二节（神经网络与模型训练过程）1.人工神经网络基础​	ANN是张量（权重，weights）和数学运算的集合，将一个或者多个张量作为输入并预测相应输出。将输入连接到输出的操作方式称为神经网络的架构，我们可以根据不同的任务构建不同架构，即基于问题是包含结构化数据还是非结构化数据（图像、文本、语言）数据，这些数据就是输入和输出张量的列表。ANN由以下部分组成： ​	输入层：将自变量作为输入 ​">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-12T14:02:16.000Z">
<meta property="article:modified_time" content="2025-09-13T02:42:13.497Z">
<meta property="article:author" content="zy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习第二节（神经网络与模型训练过程）",
  "url": "http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2025-09-12T14:02:16.000Z",
  "dateModified": "2025-09-13T02:42:13.497Z",
  "author": [
    {
      "@type": "Person",
      "name": "zy",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习第二节（神经网络与模型训练过程）',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/modify.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80);"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">zy de 小破站</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习第二节（神经网络与模型训练过程）</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">深度学习第二节（神经网络与模型训练过程）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-09-12T14:02:16.000Z" title="Created 2025-09-12 22:02:16">2025-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-09-13T02:42:13.497Z" title="Updated 2025-09-13 10:42:13">2025-09-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="深度学习第二节（神经网络与模型训练过程）"><a href="#深度学习第二节（神经网络与模型训练过程）" class="headerlink" title="深度学习第二节（神经网络与模型训练过程）"></a>深度学习第二节（神经网络与模型训练过程）</h1><h2 id="1-人工神经网络基础"><a href="#1-人工神经网络基础" class="headerlink" title="1.人工神经网络基础"></a>1.人工神经网络基础</h2><p>​	ANN是张量（权重，weights）和数学运算的集合，将一个或者多个张量作为输入并预测相应输出。将输入连接到输出的操作方式称为神经网络的架构，我们可以根据不同的任务构建不同架构，即基于问题是包含结构化数据还是非结构化数据（图像、文本、语言）数据，这些数据就是输入和输出张量的列表。ANN由以下部分组成：</p>
<p>​	输入层：将自变量作为输入</p>
<p>​	隐藏层：连接输入和输出，在输入数据之上进行转换；此外，隐藏层利用节点单元将输入值修改为更高&#x2F;更低维的值；通过修改中间节点的激活函数可以实现复杂表示函数。</p>
<p>​	输出层：输入变量产生的值，取决于实际任务以及我们是在尝试预测连续变量还是分类变量。如果输出是连续变量，则输出有一个节点。如果输出是具有m个可能类别的分类，则输出有m个节点</p>
<p>典型结构为：<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912201532438.png" alt="image-20250912201532438" style="zoom: 50%;" /></p>
<h3 id="1-1神经网络的训练"><a href="#1-1神经网络的训练" class="headerlink" title="1.1神经网络的训练"></a>1.1神经网络的训练</h3><p>​	训练神经网络实际上是通过重复两个关键步骤来调整神经网络中的权重：前向传播和反向传播。</p>
<p>1.在前向传播中，我们将一组权重应用与输入数据，将其传递给隐藏层，对隐藏层计算后的输出使用非线性激活，通过若干个隐藏层后，将最后一个隐藏层的输出与另一组权重相乘，就可以得到输出层的结果。对于第一次正向传播，权重的值将随机初始化。</p>
<p>2.反向传播中，尝试通过测量输出的误差，然后相应调整权重以降低误差。神经网络重复正向和反向传播以预测输出，直到获得令误差较小的权重为止。</p>
<h3 id="1-2常用的激活函数"><a href="#1-2常用的激活函数" class="headerlink" title="1.2常用的激活函数"></a>1.2常用的激活函数</h3><p>​	激活函数有助于对输入和输出之间的复杂关系进行建模，使用他们可以实现高度非线性。常用的激活函数如下：</p>
<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912202912683.png" alt="image-20250912202912683" style="zoom:80%;" />

<p>​	应用激活函数之后，输入值对应的激活可视化如下：</p>
<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912202948739.png" alt="image-20250912202948739" style="zoom: 67%;" />

<h3 id="1-3计算损失值"><a href="#1-3计算损失值" class="headerlink" title="1.3计算损失值"></a>1.3计算损失值</h3><p>​	损失值，也称之为成本函数，是我们需要在神经网络中优化的值。分为两种情况：分类（离散）变量预测和连续变量预测</p>
<h4 id="1-3-1在连续变量预测过程中计算损失"><a href="#1-3-1在连续变量预测过程中计算损失" class="headerlink" title="1.3.1在连续变量预测过程中计算损失"></a>1.3.1在连续变量预测过程中计算损失</h4><p>​	通常当变量为连续的时候，可以计算实际值和预测值之差的平方的平均值作为损失值，也就是改变与神经网络相关的权重值来最小化均方误差。</p>
<p><img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912203748085.png" alt="image-20250912203748085"></p>
<h4 id="1-3-2在分类（离散）变量预测计算过程中计算损失"><a href="#1-3-2在分类（离散）变量预测计算过程中计算损失" class="headerlink" title="1.3.2在分类（离散）变量预测计算过程中计算损失"></a>1.3.2在分类（离散）变量预测计算过程中计算损失</h4><p>​	当要预测的变量是离散的时候，我们通常使用分类交叉熵损失函数，当要预测的变量有两个不同的值时，损失函数为二元交叉熵。二元交叉熵计算如下：</p>
<p><img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912204016629.png" alt="image-20250912204016629"></p>
<h2 id="2-实现前向传播"><a href="#2-实现前向传播" class="headerlink" title="2.实现前向传播"></a>2.实现前向传播</h2><p>(本文中的使用numpy构建神经网络不是最佳方法）)</p>
<p>1.通过将输入值乘以权重来神经元输出值</p>
<p>2.计算激活值</p>
<p>3.在每个神经元上重复前两个步骤，直到输出层</p>
<p>4.将预测输出与真实值进行比较计算损失值</p>
<p>将以上过程封装成一个函数，将输入数据、当前神经网络权重和真是值作为函数输入，并返回网络的当前损失值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">def feed_forward(inputs,outputs,weights):</span><br><span class="line">	pre_hidden = np.dot(inputs,weights[0])+weights[1]</span><br><span class="line">	hidden = 1/(1+np.exp(-pre_hidden))#sigmod激活函数</span><br><span class="line">	pred_out = np.dot(hidden,weights[2])+weights[3]</span><br><span class="line">	mean_squared_error = np.mean(np.square(pred_out-outputs))#均方误差</span><br><span class="line">	return mean_squared_error</span><br></pre></td></tr></table></figure>

<p>​	由于为每个神经元节点添加偏置项，因此权重数组不仅包含连接不同节点的权重，还包含与隐藏层&#x2F;输出层中的节点相关的偏置项</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pre_hidden = np.dot(inputs,weights[0])+weights[1]</span><br></pre></td></tr></table></figure>

<p>​	通过执行输入层和权重层（weights[0])的矩阵乘法来计算隐藏层值，并将偏置（weights[1]）添加到隐藏层中，利用权重和偏置值就可以将输入层连接到隐藏层。</p>
<h2 id="3-实现反向传播"><a href="#3-实现反向传播" class="headerlink" title="3.实现反向传播"></a>3.实现反向传播</h2><p>​	反向传播中利用前向传播中计算的损失值，尽可能最小化损失值为目标更新网络权重。步骤如下：</p>
<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912215502325.png" alt="image-20250912215502325" style="zoom:67%;" />

<p>​	在整个数据集上执行n次上述过程（包括前向传播和反向传播），表示模型进行了n个epoch的训练（执行一次称为一个epoch）</p>
<p>​	由于神经网络可能包含数以百万的权重，因此更改每个权重的值，并检查损失的变化在实践中并不是最佳方法。上述步骤的核心就是计算权重变化的损失变化，即计算损失值关于权重的梯度。</p>
<h3 id="3-1梯度下降"><a href="#3-1梯度下降" class="headerlink" title="3.1梯度下降"></a>3.1梯度下降</h3><p>​	更新权重以减少误差值的整个过程称为梯度下降，随机梯度下降（SGD）是将误差最小化的一种方法，其中随机表示随机选择数据集中训练数据样本，并根据样本做出决策。除了随机梯度下降外，还有许多别的优化器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#定义前馈网络并计算均方误差损失值</span><br><span class="line">from copy import deepcopy</span><br><span class="line">import numpy as np</span><br><span class="line">def feed_forward(inputs,outputs,weights):</span><br><span class="line">	pre_hidden = np.dot(inputs,weights[0])+weights[1]</span><br><span class="line">	hidden = 1/(1+np.exp(-pre_hidden))#sigmod激活函数</span><br><span class="line">	pred_out = np.dot(hidden,weights[2])+weights[3]</span><br><span class="line">	mean_squared_error = np.mean(np.square(pred_out-outputs))#均方误差</span><br><span class="line">	return mean_squared_error</span><br></pre></td></tr></table></figure>

<p>1.为每一个权重和偏置项增加一个非常小的值（0.0001），并针对每个权重和偏差的更新计算一个平方误差损失值。（学习率lr）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def update_weights(inputs,outputs,weights,lr):</span><br></pre></td></tr></table></figure>

<p>2.由于权重需要在后续步骤中进行操作，因此使用deepcopy确保我们可以处理多个权重副本，而不会影响实际权重，创建作为输入传递给函数的原始权重集的三个副本——original_weights,temp_weights,updated_weights:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">original_weights = deepcopy(weights)</span><br><span class="line">temp_weights = deepcopy(weights)</span><br><span class="line">updated_weights = deepcopy(weights)</span><br></pre></td></tr></table></figure>

<p>3.使用原始权重集计算损失值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">original_loss = feed_forward(inputs,outputs,weights)</span><br></pre></td></tr></table></figure>

<p>4.遍历网络的所有层</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">	for i,layer in enumerate(original_weights):</span><br><span class="line">		for index,weight in np.ndenumerate(layer):</span><br><span class="line">		#示例神经网络中包含四个参数列表，前两个列表分别表示将输入连接到隐藏#层的权重和偏置项参数，另外两个表示连接隐藏层和输出层的权重和偏置参数。#np.ndenumerate循环遍历给定列表中的每个参数</span><br><span class="line">		temp_weights = deepcopy(weights)</span><br><span class="line">		temp_weights[i][index] += 0.0001</span><br><span class="line">		_loss_plus = feed_forward(inputs,outputs,temp_weights)</span><br><span class="line">#将temp_weights重置为原始权重集，因为在每次迭代中，都需要更新一个参数以计算#对参数进行小量更新时的损失。</span><br><span class="line">		grad = (_loss_plus - original_loss)/(0.0001)#计算由于权重变化##引起的梯度（损失值的变化）</span><br><span class="line">		update_weights[i][index] -= grad*lr #使用学习率lr令权重变化更加#稳定</span><br><span class="line">	return updated_weights,original_loss</span><br></pre></td></tr></table></figure>

<p>​	在实践中，我们通常会一次使用一批数据应用梯度下降更新网络参数，直到再一次训练周期（epoch）内遍历所有数据点。构建模型时，批大小通常为32到1024之间。</p>
<h3 id="3-2使用链式法则实现反向传播"><a href="#3-2使用链式法则实现反向传播" class="headerlink" title="3.2使用链式法则实现反向传播"></a>3.2使用链式法则实现反向传播</h3><p>​	当网络参数较多时，梯度下降法更新权重值需要进行大量计算来得到损失值，需要大量资源和时间。接下来介绍如何利用链式法则来获取与权重值有关的损失梯度。</p>
<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912215200977.png" alt="image-20250912215200977" style="zoom: 67%;" />

<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912215420281.png" alt="image-20250912215420281" style="zoom:50%;" />

<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250912215034916.png" alt="image-20250912215034916" style="zoom:50%;" />

<h2 id="3-合并前向传播和反向传播"><a href="#3-合并前向传播和反向传播" class="headerlink" title="3.合并前向传播和反向传播"></a>3.合并前向传播和反向传播</h2><p>​	实现一个简单的数据集，模型定义如下：输入连接到具有三个神经元（节点）的隐藏层，隐藏层连接到具有一个神经元的输出层。</p>
<img src="http://t2h44ecnh.hn-bkt.clouddn.com/test/image-20250913095029576.png" alt="image-20250913095029576" style="zoom:50%;" />

<p>1.导入相关的库并定义数据集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from copy import deepcopy</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">x = np.array([[1,1]])</span><br><span class="line">y = np.array([[0]])</span><br></pre></td></tr></table></figure>

<p>2.随机初始化权重和偏置</p>
<p>​	由于隐藏层中有三个神经元，每个输入节点都连接到隐藏层单元。因此共有六个权重值和三个偏置值，也就是说每个隐藏层神经元包含一个偏置和两个权重（每个输入到隐藏层神经元的连接都对应一个权重）；最后一层有一个神经元连接到隐藏层的三个d单 元，因此包含三个权重和一个偏置值。随机初始化：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">W = [</span><br><span class="line">	np.array([[-0.0052,0.3793],</span><br><span class="line">			[-0.5820,-0.5204],</span><br><span class="line">			[-0.2723,0.1896]],dtype = np.float32).T,</span><br><span class="line">	np.array([-0.0140,0.5607,-0.1135],dtype = np.float32).T,</span><br><span class="line">	np.array([[0.1528,-0.1745,-0.1135]],dtype = np.float32).T,</span><br><span class="line">	np.array([-0.5516],dtype = np.float32)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>3.在100个epoch内执行前向传播和反向传播，使用以上部分定义的feed_forward和update_weights函数。训练期间，更新权重并获取损失值和更新后的权重值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def feed_forward(inputs,outputs,weights):</span><br><span class="line">	pre_hidden = np.dot(inputs,weights[0])+weights[1]</span><br><span class="line">	hidden = 1/(1+np.exp(-pre_hidden))</span><br><span class="line">	out = np.dot(hidden,weights[2])+weights[3]</span><br><span class="line">	mean_squared_error = np.mean(np.square(out-outputs))#MSE均方误差</span><br><span class="line">	return mean_squared_error</span><br><span class="line">	</span><br><span class="line">def update_weights(inputs,outputs,weights,lr):</span><br><span class="line">	original_weights = deepcopy(weights)</span><br><span class="line">	#temp_weights = deepcopy(weights)</span><br><span class="line">	updated_weights = deepcopy(weights)</span><br><span class="line">	original_loss = feed_forward(inputs,outputs,original_weights)</span><br><span class="line">	for i ,layer in enumerate(original_weights):</span><br><span class="line">		for index,weights in np.ndenumerate(layer):</span><br><span class="line">			temp_weights = deepcopy(original_weights)</span><br><span class="line">			temp_weights[i][index] += 0.0001</span><br><span class="line">			_loss_plus = feed_forward(inputs,outputs,temp_weights)</span><br><span class="line">			grad = (_loss_plus - original_loss)/(0.0001)</span><br><span class="line">			updated_weights[i][index] -= grad*lr	</span><br><span class="line">	return updated_weights,original_loss</span><br></pre></td></tr></table></figure>

<p>4.绘制损失值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">losses = []</span><br><span class="line">for epoch in range(100):</span><br><span class="line">	W,loss = update_weights(x,y,W,0.01)</span><br><span class="line">	losses.append(loss)</span><br><span class="line">plt.plot(losses)</span><br><span class="line">plt.title(&#x27;Loss over increasing number of epochs&#x27;)</span><br><span class="line">plt.xlabel(&#x27;Epochs&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Loss value&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>5.获取更新的权重后，通过将输入传递给网络进行预测并计算输出值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(W)</span><br><span class="line">pre_hidden = np.dot(x,W[0]) + W[1]</span><br><span class="line">hidden = 1/(1+np.exp(-pre_hidden))</span><br><span class="line">pred_out = np.dot(hidden,W[2]) + W[3]</span><br><span class="line">print(pred_out)</span><br></pre></td></tr></table></figure>

<h2 id="4-神经网络训练过程总结"><a href="#4-神经网络训练过程总结" class="headerlink" title="4.神经网络训练过程总结"></a>4.神经网络训练过程总结</h2><p>​	训练神经网络主要通过两个关键步骤，即以给定的学习率进行前向传播和反向传播，最终神经网络架构得到最佳权重。需要注意的是第一次前向传播时，权重的值是随机初始化的。在反向传播中，通过在损失减少的方向上调整权重来减少损失值（误差），权重更新的幅度等于梯度乘以学习率。重复前向传播和反向传播的过程，知道获得尽可能小的损失，在训练结束时，神经网络已经将权重调整到近似最优值，以便获取期望的输出结果。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">zy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/">http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/" title="深度学习第一节"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">深度学习第一节</div></div><div class="info-2"><div class="info-item-1">深度学习第一节​	pytorch是广泛应用于机器学习领域的强大开源框架，因其易用性和高效性备受青睐。其中   pytorch的核心数据类型是张量对象。 pytorch提供了许多帮助构建神经网络的高级方法及组件，并提供了利用GPU更快的训练神经网络的张量对象。 ​	张量类似于Numpy中的多为矩阵ndarrays，标量可以表示为零维张量，向量可以表示为一维张量，二维矩阵可以表示为二维矩阵，多维矩阵可以表示为多维张量。 1PyTorch 中的张量（Tensor）和 NumPy 中的 ndarray相比，pytorch张量对象经过优化配合gpu使用。  数据基本操作1.导入pytorch并通过在列表上调用torch.tensor来初始化张量123import torchx = torch.tensor([[1,2]])y = torch.tensor([[1],[2]])  2.内置函数快速生成张量123456789101112131415161718192021222324252627282930313233343536373839x = torch.arange(12)#tenso...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zy</div><div class="author-info-description">等风来不如追风去</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">深度学习第二节（神经网络与模型训练过程）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="toc-number">1.1.</span> <span class="toc-text">1.人工神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1神经网络的训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2%E5%B8%B8%E7%94%A8%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2常用的激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1%E5%80%BC"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3计算损失值</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1%E5%9C%A8%E8%BF%9E%E7%BB%AD%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">1.3.1在连续变量预测过程中计算损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2%E5%9C%A8%E5%88%86%E7%B1%BB%EF%BC%88%E7%A6%BB%E6%95%A3%EF%BC%89%E5%8F%98%E9%87%8F%E9%A2%84%E6%B5%8B%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">1.3.2在分类（离散）变量预测计算过程中计算损失</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%9E%E7%8E%B0%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.2.</span> <span class="toc-text">2.实现前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.3.</span> <span class="toc-text">3.实现反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2使用链式法则实现反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%90%88%E5%B9%B6%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.4.</span> <span class="toc-text">3.合并前向传播和反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">4.神经网络训练过程总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/" title="深度学习第二节（神经网络与模型训练过程）">深度学习第二节（神经网络与模型训练过程）</a><time datetime="2025-09-12T14:02:16.000Z" title="Created 2025-09-12 22:02:16">2025-09-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/" title="深度学习第一节">深度学习第一节</a><time datetime="2025-09-12T09:39:03.000Z" title="Created 2025-09-12 17:39:03">2025-09-12</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By zy</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 190px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 160px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>