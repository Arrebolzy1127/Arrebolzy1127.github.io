<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习第一节 | zy de 小破站</title><meta name="author" content="zy"><meta name="copyright" content="zy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习第一节​	pytorch是广泛应用于机器学习领域的强大开源框架，因其易用性和高效性备受青睐。其中   pytorch的核心数据类型是张量对象。 pytorch提供了许多帮助构建神经网络的高级方法及组件，并提供了利用GPU更快的训练神经网络的张量对象。 ​	张量类似于Numpy中的多为矩阵ndarrays，标量可以表示为零维张量，向量可以表示为一维张量，二维矩阵可以表示为二维矩阵，多维矩阵可">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习第一节">
<meta property="og:url" content="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/index.html">
<meta property="og:site_name" content="zy de 小破站">
<meta property="og:description" content="深度学习第一节​	pytorch是广泛应用于机器学习领域的强大开源框架，因其易用性和高效性备受青睐。其中   pytorch的核心数据类型是张量对象。 pytorch提供了许多帮助构建神经网络的高级方法及组件，并提供了利用GPU更快的训练神经网络的张量对象。 ​	张量类似于Numpy中的多为矩阵ndarrays，标量可以表示为零维张量，向量可以表示为一维张量，二维矩阵可以表示为二维矩阵，多维矩阵可">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-12T09:39:03.000Z">
<meta property="article:modified_time" content="2025-09-12T09:39:20.081Z">
<meta property="article:author" content="zy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习第一节",
  "url": "http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2025-09-12T09:39:03.000Z",
  "dateModified": "2025-09-12T09:39:20.081Z",
  "author": [
    {
      "@type": "Person",
      "name": "zy",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习第一节',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/modify.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-4.0.3&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80);"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">zy de 小破站</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习第一节</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">深度学习第一节</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-09-12T09:39:03.000Z" title="Created 2025-09-12 17:39:03">2025-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-09-12T09:39:20.081Z" title="Updated 2025-09-12 17:39:20">2025-09-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="深度学习第一节"><a href="#深度学习第一节" class="headerlink" title="深度学习第一节"></a>深度学习第一节</h1><p>​	pytorch是广泛应用于机器学习领域的强大开源框架，因其易用性和高效性备受青睐。其中   pytorch的核心数据类型是张量对象。 pytorch提供了许多帮助构建神经网络的高级方法及组件，并提供了利用GPU更快的训练神经网络的张量对象。</p>
<p>​	张量类似于Numpy中的多为矩阵ndarrays，标量可以表示为零维张量，向量可以表示为一维张量，二维矩阵可以表示为二维矩阵，多维矩阵可以表示为多维张量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyTorch 中的张量（Tensor）和 NumPy 中的 ndarray相比，pytorch张量对象经过优化配合gpu使用。</span><br></pre></td></tr></table></figure>

<h2 id="数据基本操作"><a href="#数据基本操作" class="headerlink" title="数据基本操作"></a>数据基本操作</h2><h3 id="1-导入pytorch并通过在列表上调用torch-tensor来初始化张量"><a href="#1-导入pytorch并通过在列表上调用torch-tensor来初始化张量" class="headerlink" title="1.导入pytorch并通过在列表上调用torch.tensor来初始化张量"></a>1.导入pytorch并通过在列表上调用torch.tensor来初始化张量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.tensor([[1,2]])</span><br><span class="line">y = torch.tensor([[1],[2]])</span><br></pre></td></tr></table></figure>

<h3 id="2-内置函数快速生成张量"><a href="#2-内置函数快速生成张量" class="headerlink" title="2.内置函数快速生成张量"></a>2.内置函数快速生成张量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(12)</span><br><span class="line">#tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br><span class="line"></span><br><span class="line">x = torch.zeros((2,3,4))</span><br><span class="line">#tensor([[[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]]])</span><br><span class="line">         </span><br><span class="line">x = torch.ones((2,3,4))</span><br><span class="line">#tensor([[[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]]])</span><br><span class="line">         </span><br><span class="line">#随机填充</span><br><span class="line">c = torch.randint(low=0,high=10,size=(3,4))</span><br><span class="line">#每个元素为0到10之间的随机整数值</span><br><span class="line">#tensor([[4, 5, 5, 4],</span><br><span class="line">         [1, 5, 3, 2],</span><br><span class="line">         [3, 5, 7, 6]])</span><br><span class="line"></span><br><span class="line">d = torch.rand(3,4)</span><br><span class="line">#每个元素为0到1之间的随机浮点值</span><br><span class="line">#tensor([[0.0413, 0.9341, 0.4687, 0.3344],</span><br><span class="line">         [0.1153, 0.6723, 0.3727, 0.3245],</span><br><span class="line">         [0.6182, 0.1326, 0.9461, 0.5833]])</span><br><span class="line"></span><br><span class="line">e = torch.randn((3,4))</span><br><span class="line"># 每个元素服从正态分布</span><br><span class="line">#tensor([[ 0.1260,  1.5217, -0.0127,  2.3229],</span><br><span class="line">         [ 0.5907,  0.1080, -0.5392,  0.9487],</span><br><span class="line">         [ 1.0874, -0.0342,  0.9897, -1.0400]])</span><br></pre></td></tr></table></figure>

<h3 id="3-获取张量对象的形状和数据类型"><a href="#3-获取张量对象的形状和数据类型" class="headerlink" title="3.获取张量对象的形状和数据类型"></a>3.获取张量对象的形状和数据类型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br><span class="line">x.dtype</span><br></pre></td></tr></table></figure>

<p>注意：同一张量的所有元素的数据类型相同，这意味着如果张量包含不同数据类型的数据（例如布尔、整数和浮点数），则整个张量被强制转换为最通用的数据类型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([False,1,2.0])</span><br><span class="line">print(x)</span><br><span class="line">#tensor([0.,1.,2.])</span><br></pre></td></tr></table></figure>

<h3 id="4-numpy中-ndarray可以转换为tensor"><a href="#4-numpy中-ndarray可以转换为tensor" class="headerlink" title="4.numpy中 ndarray可以转换为tensor"></a>4.numpy中 ndarray可以转换为tensor</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line">#type(A),type(B)</span><br><span class="line">#(numpy.ndarray, torch.Tensor)</span><br><span class="line"></span><br><span class="line">注意：只有一个元素可以转换为标量</span><br><span class="line">a = torch.tensor([3.5])</span><br><span class="line">#a,a.item,float(a),int(a)</span><br><span class="line">#tensor([3.5000]), &lt;function Tensor.item&gt;, 3.5, 3</span><br></pre></td></tr></table></figure>

<h3 id="5-原地操作内存地址不变"><a href="#5-原地操作内存地址不变" class="headerlink" title="5.原地操作内存地址不变"></a>5.原地操作内存地址不变</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.arange(12)</span><br><span class="line">before = id(Y)</span><br><span class="line">Y = x+y</span><br><span class="line">#id(Y) == before</span><br><span class="line">#False</span><br><span class="line">#内存地址变化</span><br><span class="line"></span><br><span class="line">Y = torch.arange(12).reshape((2,6))</span><br><span class="line">before = id(Y)</span><br><span class="line">X = x.reshape((2,6))</span><br><span class="line">Y[:] = X+Y</span><br><span class="line">#id(Y) == before</span><br><span class="line">#True</span><br><span class="line">#原地操作内存地址不变化</span><br></pre></td></tr></table></figure>

<h3 id="6-张量运算"><a href="#6-张量运算" class="headerlink" title="6.张量运算"></a>6.张量运算</h3><p>​		神经网络中常见运算包括输入与权重的矩阵相乘，添加偏置项，以及需要时整形（reshape）输入或权重值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">每个元素×10，每个元素+10</span><br><span class="line">A = torch.tensor([[1,2,3,4],[3,4,5,6]])</span><br><span class="line">#A*10,A+10,A.add(10)</span><br><span class="line">#tensor([[10, 20, 30, 40],</span><br><span class="line">         [30, 40, 50, 60]]),</span><br><span class="line"> tensor([[11, 12, 13, 14],</span><br><span class="line">         [13, 14, 15, 16]]),</span><br><span class="line"> tensor([[11, 12, 13, 14],</span><br><span class="line">         [13, 14, 15, 16]])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#矩阵乘法</span><br><span class="line">torch.matmul(x,y)</span><br><span class="line">x@y</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x==y</span><br><span class="line">#tensor([[False,  True, False,  True],</span><br><span class="line">        [False, False, False, False],</span><br><span class="line">        [False, False, False, False]])</span><br></pre></td></tr></table></figure>

<h3 id="7-改变张量形状"><a href="#7-改变张量形状" class="headerlink" title="7.改变张量形状"></a>7.改变张量形状</h3><p>​	在 PyTorch 中，reshape、view和 squeeze都是用于改变张量形状（shape）的操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">torch.reshape()</span><br><span class="line">#reshape 函数会返回一个具有指定形状的新张量。它既可以操作连续的张量，也可以操作不</span><br><span class="line">连续的张量。</span><br><span class="line">#可能复制数据，如果张量不连续则会复制数据</span><br><span class="line"></span><br><span class="line">troch.view()</span><br><span class="line">#只能操作连续的张量</span><br><span class="line">#不复制数据，返回一个共享底层数据的视图</span><br><span class="line"></span><br><span class="line">torch.squeeze()</span><br><span class="line">#移除张量中所有或指定维度中大小为 1 的维度</span><br><span class="line">#如果不指定维度，squeeze() 会移除所有大小为 1 的维度。</span><br><span class="line">#如果指定维度 dim，squeeze(dim) 只会移除该维度，前提是该维度的大小为 1。如果指定维度的大小不为 1，则该操作不会改变张量的形状。</span><br><span class="line">#squeeze 操作通常返回一个共享底层数据的新视图，不进行数据复制。</span><br><span class="line">x = torch.randn(10,1,10)</span><br><span class="line">z1 = torch.squeeze(x,1)</span><br><span class="line">z2 = x.squeeze(1)</span><br><span class="line">assert torch.all(z1==z2)</span><br><span class="line">#x.shape,z1.shape</span><br><span class="line">#torch.Size([10, 1, 10]), torch.Size([10, 10])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">与squeeze相反的操作是unsqueeze，向矩阵添加一个新维度</span><br><span class="line">x = torch.randn(10,10)</span><br><span class="line">z1 = x.unsqueeze(0)</span><br><span class="line">z2,z3,z4 = x[None],x[:,None],x[:,:,None]#也可以使用None进行创建新通道/维度</span><br><span class="line">#x.shape,z1.shape,z2.shape,z3.shape</span><br><span class="line">#torch.Size([10, 10]),</span><br><span class="line"> torch.Size([1, 10, 10]),</span><br><span class="line"> torch.Size([1, 10, 10]),</span><br><span class="line"> torch.Size([10, 1, 10])</span><br></pre></td></tr></table></figure>

<h3 id="8-张量连接运算"><a href="#8-张量连接运算" class="headerlink" title="8.张量连接运算"></a>8.张量连接运算</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(12,dtype = torch.float32).reshape((3,4))</span><br><span class="line">y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])</span><br><span class="line">#torch.cat((x,y),dim=0),torch.cat((x,y),dim=1)</span><br><span class="line">#tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.],</span><br><span class="line">         [ 2.,  1.,  4.,  3.],</span><br><span class="line">         [ 1.,  2.,  3.,  4.],</span><br><span class="line">         [ 4.,  3.,  2.,  1.]]),</span><br><span class="line"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class="line">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])</span><br></pre></td></tr></table></figure>

<h3 id="9-提取张量中最大值"><a href="#9-提取张量中最大值" class="headerlink" title="9.提取张量中最大值"></a>9.提取张量中最大值</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#提取张量中的最大值</span><br><span class="line">x = torch.arange(25).reshape(5,5)</span><br><span class="line">#x.max()</span><br><span class="line">#tensor(24)</span><br><span class="line">#提取最大值以及存在最大值的行索引</span><br><span class="line">#x.max(dim=0)</span><br><span class="line">#torch.return_types.max(</span><br><span class="line">values=tensor([20, 21, 22, 23, 24]),</span><br><span class="line">indices=tensor([4, 4, 4, 4, 4]))</span><br><span class="line">#m,argm = x.max(dim=1)</span><br><span class="line">#tensor([ 4,  9, 14, 19, 24]), tensor([4, 4, 4, 4, 4])</span><br></pre></td></tr></table></figure>

<h3 id="10-张量对象的自动梯度计算"><a href="#10-张量对象的自动梯度计算" class="headerlink" title="10.张量对象的自动梯度计算"></a>10.张量对象的自动梯度计算</h3><p>​	微分和计算梯度在更新神经网络的权重中至关重要，pytorch中的张量对象内置梯度计算函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#定义一个张量对象，同时指定它需要计算梯度,requires_grad = True指定腰围张#量对象计算梯度</span><br><span class="line">x = torch.tensor([[2.,-1.],[1.,1.]],requires_grad = True)</span><br><span class="line">#tensor([[ 2., -1.],</span><br><span class="line">        [ 1.,  1.]], requires_grad=True)</span><br></pre></td></tr></table></figure>

<h3 id="11-计算所有输入的平方和"><a href="#11-计算所有输入的平方和" class="headerlink" title="11.计算所有输入的平方和"></a>11.计算所有输入的平方和</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out = x.pow(2).sum()</span><br><span class="line">#x,out</span><br><span class="line">#tensor([[ 2., -1.],</span><br><span class="line">         [ 1.,  1.]], requires_grad=True),</span><br><span class="line"> tensor(7., grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h3 id="12-计算梯度"><a href="#12-计算梯度" class="headerlink" title="12.计算梯度"></a>12.计算梯度</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad</span><br><span class="line">#tensor([[ 4., -2.],</span><br><span class="line">        [ 2.,  2.]])</span><br></pre></td></tr></table></figure>

<h3 id="13-比较numpy数组和tensor张量执行矩阵所花费的时间"><a href="#13-比较numpy数组和tensor张量执行矩阵所花费的时间" class="headerlink" title="13.比较numpy数组和tensor张量执行矩阵所花费的时间"></a>13.比较numpy数组和tensor张量执行矩阵所花费的时间</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#生成不同的torch对象</span><br><span class="line">import torch</span><br><span class="line">import time</span><br><span class="line">x = torch.rand(1,6400)</span><br><span class="line">y = torch.rand(6400,5000)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#定义用于存储张量对象的设备</span><br><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">assert device == &#x27;cuda&#x27;,&quot;this exercise is on a GPU machine&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#将创建的张量对象注册到设备中，注册张量对象意味着将信息存储到指定设备中</span><br><span class="line">x,y = x.to(device),y.to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#执行torch对象的矩阵乘法，并对其计时，并比较在numpy数组中执行矩阵乘法的速度。</span><br><span class="line">start = time.time()</span><br><span class="line">for i in range(10):</span><br><span class="line">	z = x@y</span><br><span class="line">end = time.time()</span><br><span class="line">print(end - start)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#在cpu上执行相同的矩阵乘法</span><br><span class="line">x,y = x.cpu(),y.cpu()</span><br><span class="line">start = time.time()</span><br><span class="line">for i in range(10):</span><br><span class="line">	z = x@y</span><br><span class="line">end = time.time()</span><br><span class="line">print(end - start)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#在numpy数组上执行相同的矩阵乘法</span><br><span class="line">import numpy as np</span><br><span class="line">x = np.random.random((1,6400))</span><br><span class="line">y = np.random.random((6400,5000))</span><br><span class="line">start = time.time()</span><br><span class="line">for i in range(10):</span><br><span class="line">	z = np.matmul(x,y)</span><br><span class="line">end = time.time()</span><br><span class="line">print(end - start)</span><br></pre></td></tr></table></figure>

<p>​		在gpu上对torch对象执行的矩阵乘法比在cpu上的torch对象快约18倍，比在numpy数组上执行矩阵乘法快约40倍，一般来说，在cpu中使用torch张量的矩阵乘法同样比numpy更快。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">zy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/">http://example.com/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/" title="深度学习第二节（神经网络与模型训练过程）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">深度学习第二节（神经网络与模型训练过程）</div></div><div class="info-2"><div class="info-item-1">深度学习第二节（神经网络与模型训练过程）1.人工神经网络基础​	ANN是张量（权重，weights）和数学运算的集合，将一个或者多个张量作为输入并预测相应输出。将输入连接到输出的操作方式称为神经网络的架构，我们可以根据不同的任务构建不同架构，即基于问题是包含结构化数据还是非结构化数据（图像、文本、语言）数据，这些数据就是输入和输出张量的列表。ANN由以下部分组成： ​	输入层：将自变量作为输入 ​	隐藏层：连接输入和输出，在输入数据之上进行转换；此外，隐藏层利用节点单元将输入值修改为更高&#x2F;更低维的值；通过修改中间节点的激活函数可以实现复杂表示函数。 ​	输出层：输入变量产生的值，取决于实际任务以及我们是在尝试预测连续变量还是分类变量。如果输出是连续变量，则输出有一个节点。如果输出是具有m个可能类别的分类，则输出有m个节点 典型结构为： 1.1神经网络的训练​	训练神经网络实际上是通过重复两个关键步骤来调整神经网络中的权重：前向传播和反向传播。 1.在前向传播中，我们将一组权重应用与输入数据，将其传递给隐藏层，对隐藏层计算后的输出使用非线性激活，通过若干个隐藏层后，将最后...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">zy</div><div class="author-info-description">等风来不如追风去</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82"><span class="toc-number">1.</span> <span class="toc-text">深度学习第一节</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.</span> <span class="toc-text">数据基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AF%BC%E5%85%A5pytorch%E5%B9%B6%E9%80%9A%E8%BF%87%E5%9C%A8%E5%88%97%E8%A1%A8%E4%B8%8A%E8%B0%83%E7%94%A8torch-tensor%E6%9D%A5%E5%88%9D%E5%A7%8B%E5%8C%96%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.导入pytorch并通过在列表上调用torch.tensor来初始化张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%E5%BF%AB%E9%80%9F%E7%94%9F%E6%88%90%E5%BC%A0%E9%87%8F"><span class="toc-number">1.1.2.</span> <span class="toc-text">2.内置函数快速生成张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%8E%B7%E5%8F%96%E5%BC%A0%E9%87%8F%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%BD%A2%E7%8A%B6%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.1.3.</span> <span class="toc-text">3.获取张量对象的形状和数据类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-numpy%E4%B8%AD-ndarray%E5%8F%AF%E4%BB%A5%E8%BD%AC%E6%8D%A2%E4%B8%BAtensor"><span class="toc-number">1.1.4.</span> <span class="toc-text">4.numpy中 ndarray可以转换为tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%E4%B8%8D%E5%8F%98"><span class="toc-number">1.1.5.</span> <span class="toc-text">5.原地操作内存地址不变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97"><span class="toc-number">1.1.6.</span> <span class="toc-text">6.张量运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%94%B9%E5%8F%98%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6"><span class="toc-number">1.1.7.</span> <span class="toc-text">7.改变张量形状</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%BC%A0%E9%87%8F%E8%BF%9E%E6%8E%A5%E8%BF%90%E7%AE%97"><span class="toc-number">1.1.8.</span> <span class="toc-text">8.张量连接运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E6%8F%90%E5%8F%96%E5%BC%A0%E9%87%8F%E4%B8%AD%E6%9C%80%E5%A4%A7%E5%80%BC"><span class="toc-number">1.1.9.</span> <span class="toc-text">9.提取张量中最大值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E5%BC%A0%E9%87%8F%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%87%AA%E5%8A%A8%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.1.10.</span> <span class="toc-text">10.张量对象的自动梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E8%AE%A1%E7%AE%97%E6%89%80%E6%9C%89%E8%BE%93%E5%85%A5%E7%9A%84%E5%B9%B3%E6%96%B9%E5%92%8C"><span class="toc-number">1.1.11.</span> <span class="toc-text">11.计算所有输入的平方和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.1.12.</span> <span class="toc-text">12.计算梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-%E6%AF%94%E8%BE%83numpy%E6%95%B0%E7%BB%84%E5%92%8Ctensor%E5%BC%A0%E9%87%8F%E6%89%A7%E8%A1%8C%E7%9F%A9%E9%98%B5%E6%89%80%E8%8A%B1%E8%B4%B9%E7%9A%84%E6%97%B6%E9%97%B4"><span class="toc-number">1.1.13.</span> <span class="toc-text">13.比较numpy数组和tensor张量执行矩阵所花费的时间</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E8%8A%82%EF%BC%88%E4%BD%BF%E7%94%A8Pytorch%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89/" title="深度学习第三节（使用Pytorch构建神经网络）">深度学习第三节（使用Pytorch构建神经网络）</a><time datetime="2025-09-13T09:01:40.000Z" title="Created 2025-09-13 17:01:40">2025-09-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%BA%8C%E8%8A%82%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%EF%BC%89/" title="深度学习第二节（神经网络与模型训练过程）">深度学习第二节（神经网络与模型训练过程）</a><time datetime="2025-09-12T14:02:16.000Z" title="Created 2025-09-12 22:02:16">2025-09-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%8A%82/" title="深度学习第一节">深度学习第一节</a><time datetime="2025-09-12T09:39:03.000Z" title="Created 2025-09-12 17:39:03">2025-09-12</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By zy</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 190px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 160px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>